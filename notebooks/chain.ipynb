{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install absl-py jax jaxlib dm-env dm-haiku bsuite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from typing import Callable, List, NamedTuple, Tuple, Union\n",
    "\n",
    "import dm_env\n",
    "import haiku as hk\n",
    "import jax\n",
    "import numpy as np\n",
    "from bsuite.baselines import experiment\n",
    "from bsuite.baselines.utils import sequence\n",
    "from bsuite.baselines.base import Agent\n",
    "from bsuite.environments.base import Environment\n",
    "from dm_env import specs\n",
    "from jax import numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Horizon = int\n",
    "Observation = Union[int, np.ndarray, jnp.ndarray]\n",
    "Action = int\n",
    "Features = Union[np.ndarray, jnp.ndarray]\n",
    "BasisFunction = Callable[[Horizon, Observation, Action], jnp.ndarray]\n",
    "\n",
    "\n",
    "class AgentState(NamedTuple):\n",
    "    params_mean: jnp.ndarray\n",
    "    params_cov: jnp.ndarray\n",
    "    params_sample: jnp.ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChainEnv(Environment):\n",
    "    bsuite_num_episodes = 0\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        size: int = 10,\n",
    "        deterministic: bool = False,\n",
    "        seed: int = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._size = size\n",
    "        self._deterministic = deterministic\n",
    "        self._rng = np.random.RandomState(seed)\n",
    "        if not self._deterministic:\n",
    "            self._optimal_return = (1 - 1 / self._size)**(self._size - 1)\n",
    "        else:\n",
    "            self._optimal_return = 1.\n",
    "        self._total_regret = 0\n",
    "        self._reward_seq = []\n",
    "        self._regret_seq = []\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self) -> dm_env.TimeStep:\n",
    "        self._timestep = 0\n",
    "        self._position = 0\n",
    "        return dm_env.restart(self._get_observation())\n",
    "\n",
    "    def _step(\n",
    "        self,\n",
    "        action: int,\n",
    "    ) -> dm_env.TimeStep:\n",
    "        if (action == 1) and (not self._deterministic) and (self._rng.rand() <\n",
    "                                                            1. / self._size):\n",
    "            action = 0\n",
    "        self._timestep += 1\n",
    "        self._position = max(\n",
    "            0, min(self._position + 2 * action - 1, self._size - 1))\n",
    "        observation = self._get_observation()\n",
    "        reward = int(observation == self._size - 1)\n",
    "        if self._timestep == self._size - 1:\n",
    "            self._total_regret += self._optimal_return - reward\n",
    "            self._reward_seq.append(reward)\n",
    "            self._regret_seq.append(self._total_regret)\n",
    "            self.bsuite_num_episodes += 1\n",
    "            self.print_info()\n",
    "            return dm_env.termination(reward=reward, observation=observation)\n",
    "        return dm_env.transition(reward=reward, observation=observation)\n",
    "\n",
    "    def _get_observation(self):\n",
    "        obs = self._position\n",
    "        return obs\n",
    "\n",
    "    def observation_spec(self):\n",
    "        return specs.DiscreteArray(self._size, name='position')\n",
    "\n",
    "    def action_spec(self):\n",
    "        return specs.DiscreteArray(2, name='action')\n",
    "\n",
    "    def bsuite_info(self):\n",
    "        return dict(total_regret=self._total_regret)\n",
    "\n",
    "    def print_info(self):\n",
    "        info = 'episode = {} '.format(self.bsuite_num_episodes) + '|'\n",
    "        info += 'total return = {}'.format(int(sum(self._reward_seq))) + '|'\n",
    "        info += 'total regret = {:.2f}'.format(self._total_regret) \n",
    "        print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basis function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_coherent_basis(\n",
    "    size: int,\n",
    "    deterministic: bool,\n",
    "    num_basis: int,\n",
    "    rng: hk.PRNGSequence,\n",
    ") -> BasisFunction:\n",
    "\n",
    "    S = size\n",
    "    A = 2\n",
    "    H = S - 1\n",
    "    K = num_basis\n",
    "    if deterministic:\n",
    "        p = 1.\n",
    "    else:\n",
    "        p = (1 - 1. / S)\n",
    "\n",
    "    # optimal state value function\n",
    "    aux = np.array([p**t for t in range(H, 0, -1)])\n",
    "    aux = np.tril(jnp.ones(H)) @ np.diag(aux)\n",
    "    aux = np.cumsum(aux[:, ::-1], axis=1)[:, ::-1]\n",
    "    v_opt = np.concatenate([aux, aux[-1:, :]], axis=0).T\n",
    "\n",
    "    # optimal state-action value function\n",
    "    q_right = v_opt\n",
    "    q_left = np.zeros((H, S))\n",
    "    q_left[:-1, 1:] = v_opt[1:, :-1]\n",
    "    q_opt = np.stack([q_left, q_right], axis=2)\n",
    "\n",
    "    # coherent basis\n",
    "    psi_ones = jnp.ones(shape=(H * S * A, 1))\n",
    "    psi_opt = jnp.reshape(q_opt, [H * S * A, 1])\n",
    "    psi_rand = jax.random.normal(next(rng), shape=(H * S * A, K - 2))\n",
    "    psi = jnp.concatenate([psi_ones, psi_opt, psi_rand], axis=1)\n",
    "    proj = psi @ jnp.linalg.inv(psi.T @ psi) @ psi.T\n",
    "    w_ones = jnp.ones(shape=(H * S * A, 1))\n",
    "    w_rand = jax.random.normal(next(rng), shape=(H * S * A, K - 1))\n",
    "    w = jnp.concatenate([w_ones, w_rand], axis=1)\n",
    "    w_proj = proj @ w\n",
    "    w_proj /= jnp.linalg.norm(w_proj, axis=0)\n",
    "    w_proj *= H * S * A\n",
    "    phi = w_proj.reshape(H, S, A, K)\n",
    "\n",
    "    # basis function\n",
    "    def basis_function(h, s, a):\n",
    "        return phi[h, s, a].squeeze()\n",
    "\n",
    "    return basis_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomized LSVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def _argmax(\n",
    "    f: jnp.ndarray,\n",
    "    p: jnp.ndarray,\n",
    ") -> jnp.ndarray:\n",
    "    q = jnp.matmul(f, p)\n",
    "    a = jnp.argmax(q, axis=-1)\n",
    "    return a.squeeze()\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def _max(\n",
    "    f: jnp.ndarray,\n",
    "    p: jnp.ndarray,\n",
    ") -> jnp.ndarray:\n",
    "    q = jnp.matmul(f, p)\n",
    "    q_max = jnp.max(q, axis=-1)\n",
    "    return q_max.squeeze()\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def _sample(\n",
    "    rng: hk.PRNGSequence,\n",
    "    m: jnp.ndarray,\n",
    "    c: jnp.ndarray,\n",
    ") -> jnp.ndarray:\n",
    "    s = jax.random.multivariate_normal(rng, m, c)\n",
    "    return s\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def _lstsq(\n",
    "    A: jnp.ndarray,\n",
    "    b: jnp.ndarray,\n",
    "    reg_parameter: float,\n",
    ") -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
    "\n",
    "    # get parameters covariance\n",
    "    inv_cov = jnp.matmul(A.T, A) + reg_parameter * jnp.eye(A.shape[1])\n",
    "    cov = jnp.linalg.inv(inv_cov)\n",
    "\n",
    "    # get parameters mean\n",
    "    mean = jnp.dot(jnp.matmul(cov, A.T), b)\n",
    "\n",
    "    return mean, cov\n",
    "\n",
    "\n",
    "def _rlsvi(\n",
    "    agent_state: AgentState,\n",
    "    basis_function: BasisFunction,\n",
    "    trajectories: List[sequence.Trajectory],\n",
    "    noise_std: float,\n",
    "    reg_parameter: float,\n",
    "    sequence_length: int,\n",
    "    rng: hk.PRNGSequence,\n",
    ") -> AgentState:\n",
    "\n",
    "    basis_function = jax.vmap(basis_function)\n",
    "\n",
    "    observations = jnp.stack([t.observations for t in trajectories])\n",
    "    actions = jnp.stack([t.actions for t in trajectories])\n",
    "    rewards = jnp.stack([t.rewards for t in trajectories])\n",
    "\n",
    "    params_mean, params_cov, params_sample = [], [], []\n",
    "\n",
    "    for h in range(sequence_length)[::-1]:\n",
    "\n",
    "        # get regression matrix\n",
    "        batch_h = h * jnp.ones(len(observations), dtype=int)\n",
    "        batch_o = observations[:, h]\n",
    "        batch_a = actions[:, h]\n",
    "        batch_f = basis_function(batch_h, batch_o, batch_a)\n",
    "        A = batch_f / noise_std\n",
    "\n",
    "        # get regression vector\n",
    "        batch_r = rewards[:, h]\n",
    "        batch_next_o = observations[:, h + 1]\n",
    "        if h == sequence_length - 1:\n",
    "            batch_next_r = rewards[:, -1]\n",
    "        else:\n",
    "            batch_next_f = basis_function(batch_h + 1, batch_next_o, None)\n",
    "            next_p = params_sample[-1]\n",
    "            batch_next_r = _max(batch_next_f, next_p)\n",
    "        b = (batch_r + batch_next_r) / noise_std\n",
    "\n",
    "        # get parameters\n",
    "        mean, cov = _lstsq(A, b, reg_parameter)\n",
    "        sample = _sample(next(rng), mean, cov)\n",
    "        params_mean.append(mean)\n",
    "        params_cov.append(cov)\n",
    "        params_sample.append(sample)\n",
    "\n",
    "    return agent_state._replace(params_mean=params_mean[::-1],\n",
    "                                params_cov=params_cov[::-1],\n",
    "                                params_sample=params_sample[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLSVI(Agent):\n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_spec: specs.Array,\n",
    "        action_spec: specs.DiscreteArray,\n",
    "        basis_function: BasisFunction,\n",
    "        sequence_length: int,\n",
    "        noise_std: float,\n",
    "        reg_parameter: float,\n",
    "        rng: hk.PRNGSequence,\n",
    "    ):\n",
    "\n",
    "        self._obs_spec = obs_spec\n",
    "        self._action_spec = action_spec\n",
    "        self._sequence_length = sequence_length\n",
    "        self._basis_function = basis_function\n",
    "        self._noise_std = noise_std\n",
    "        self._reg_parameter = reg_parameter\n",
    "        self._rng = rng\n",
    "\n",
    "        # initialize buffer\n",
    "        self._trajectories = []\n",
    "        self._buffer = sequence.Buffer(obs_spec, action_spec, sequence_length)\n",
    "\n",
    "        # initialize parameters\n",
    "        self._state = AgentState(None, None, None)\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        timestep: dm_env.TimeStep,\n",
    "        action: Action,\n",
    "        new_timestep: dm_env.TimeStep,\n",
    "    ):\n",
    "\n",
    "        self._buffer.append(timestep, action, new_timestep)\n",
    "        if self._buffer.full() or new_timestep.last():\n",
    "            trajectory = self._buffer.drain()\n",
    "            self._trajectories.append(deepcopy(trajectory))\n",
    "            self._state = _rlsvi(self._state, self._basis_function,\n",
    "                                 self._trajectories, self._noise_std,\n",
    "                                 self._reg_parameter, self._sequence_length,\n",
    "                                 self._rng)\n",
    "\n",
    "    def select_action(\n",
    "        self,\n",
    "        timestep: dm_env.TimeStep,\n",
    "    ) -> Action:\n",
    "\n",
    "        if self._state.params_sample == None:\n",
    "            action = jax.random.randint(next(self._rng), (), 0,\n",
    "                                        self._action_spec.num_values)\n",
    "            return int(action)\n",
    "\n",
    "        h = int(timestep.step_type)\n",
    "        o = timestep.observation\n",
    "        f = jnp.stack([\n",
    "            self._basis_function(h, o, a)\n",
    "            for a in range(self._action_spec.num_values)\n",
    "        ],\n",
    "                      axis=0)\n",
    "        p = self._state.params_sample[h]\n",
    "        action = _argmax(f, p)\n",
    "\n",
    "        return int(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234 # Random seed\n",
    "size = 20 # Size of chain experiment\n",
    "deterministic = False # Deterministic environment\n",
    "num_basis = 10 # Number of basis functions\n",
    "noise_std = 0.1 # Standard deviation of RLSVI\n",
    "reg_parameter = 1. # Regularization parameter of RLSVI\n",
    "num_episodes = 300 # Number of train episodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(seed)\n",
    "rng = hk.PRNGSequence(key)\n",
    "\n",
    "# set environment\n",
    "env = ChainEnv(size=size, deterministic=deterministic, seed = seed)\n",
    "\n",
    "# set basis function\n",
    "basis_function = random_coherent_basis(size=size,\n",
    "                                        deterministic=deterministic,\n",
    "                                        num_basis=num_basis,\n",
    "                                        rng=rng)\n",
    "\n",
    "# set agent\n",
    "agent = RLSVI(obs_spec=env.observation_spec(),\n",
    "                action_spec=env.action_spec(),\n",
    "                basis_function=basis_function,\n",
    "                sequence_length=size,\n",
    "                noise_std=noise_std,\n",
    "                reg_parameter=reg_parameter,\n",
    "                rng=rng)\n",
    "\n",
    "# run experiment\n",
    "experiment.run(agent=agent,\n",
    "                environment=env,\n",
    "                num_episodes=num_episodes,\n",
    "                verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "x = range(env.bsuite_num_episodes)\n",
    "y = env._regret_seq\n",
    "\n",
    "ax.plot(x,y)\n",
    "\n",
    "ax.set_xlabel('Episodes')\n",
    "ax.set_ylabel('Cumulative regret')\n",
    "ax.set_title('Chain with {} states'.format(size))\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "katqG1libWKj"
   ],
   "name": "chain.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
