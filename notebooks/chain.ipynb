{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python374jvsc74a57bd05c66cf5cbfaa7f8d834355f867a75c06b310cfb9e2d21fde300b50fc812898c6",
      "display_name": "Python 3.7.4 64-bit ('base': conda)"
    },
    "colab": {
      "name": "chain.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "katqG1libWKj"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4eXdi5YbWKe"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Hul3MnfbWKh"
      },
      "source": [
        "!pip install absl-py jax jaxlib dm-env dm-haiku bsuite"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "katqG1libWKj"
      },
      "source": [
        "# Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhe5DG6IbWKk"
      },
      "source": [
        "from copy import deepcopy\n",
        "from functools import partial\n",
        "from typing import Callable, List, NamedTuple, Tuple\n",
        "\n",
        "import dm_env\n",
        "import haiku as hk\n",
        "import jax\n",
        "import numpy as np\n",
        "from bsuite.baselines import experiment\n",
        "from bsuite.baselines.utils import sequence\n",
        "from bsuite.baselines.base import Agent\n",
        "from bsuite.environments.base import Environment\n",
        "from dm_env import specs\n",
        "from jax import numpy as jnp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeIc7w6Abjyw"
      },
      "source": [
        "Action = int\n",
        "BasisFunction = Callable[[jnp.ndarray, jnp.ndarray, jnp.ndarray], jnp.ndarray]\n",
        "\n",
        "\n",
        "class AgentState(NamedTuple):\n",
        "    params_mean: jnp.ndarray\n",
        "    params_cov: jnp.ndarray\n",
        "    params_sample: jnp.ndarray"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-BsI6b_bWKk"
      },
      "source": [
        "# Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUj7u5yqbWKm"
      },
      "source": [
        "class ChainEnv(Environment):\n",
        "    bsuite_num_episodes = 0\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        size: int = 10,\n",
        "        deterministic: bool = False,\n",
        "        seed: int = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self._size = size\n",
        "        self._deterministic = deterministic\n",
        "        self._rng = np.random.RandomState(seed)\n",
        "        if not self._deterministic:\n",
        "            self._optimal_return = (1 - 1 / self._size)**(self._size - 1)\n",
        "        else:\n",
        "            self._optimal_return = 1.\n",
        "        self._total_regret = 0\n",
        "        self._reward_seq = []\n",
        "        self._regret_seq = []\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self) -> dm_env.TimeStep:\n",
        "        self._timestep = 0\n",
        "        self._position = 0\n",
        "        return dm_env.restart(self._get_observation())\n",
        "\n",
        "    def _step(\n",
        "        self,\n",
        "        action: int,\n",
        "    ) -> dm_env.TimeStep:\n",
        "        if (action == 1) and (not self._deterministic) and (self._rng.rand() <\n",
        "                                                            1. / self._size):\n",
        "            action = 0\n",
        "        self._timestep += 1\n",
        "        self._position = max(\n",
        "            0, min(self._position + 2 * action - 1, self._size - 1))\n",
        "        observation = self._get_observation()\n",
        "        reward = int(observation == self._size - 1)\n",
        "        if self._timestep == self._size - 1:\n",
        "            self._total_regret += self._optimal_return - reward\n",
        "            self._reward_seq.append(reward)\n",
        "            self._regret_seq.append(self._total_regret)\n",
        "            self.bsuite_num_episodes += 1\n",
        "            self.print_info()\n",
        "            \n",
        "            return dm_env.termination(reward=reward, observation=observation)\n",
        "        return dm_env.transition(reward=reward, observation=observation)\n",
        "\n",
        "    def _get_observation(self):\n",
        "        obs = self._position\n",
        "        return obs\n",
        "\n",
        "    def observation_spec(self):\n",
        "        return specs.DiscreteArray(self._size, name='position')\n",
        "\n",
        "    def action_spec(self):\n",
        "        return specs.DiscreteArray(2, name='action')\n",
        "\n",
        "    def bsuite_info(self):\n",
        "        return dict(total_regret=self._total_regret)\n",
        "\n",
        "    def print_info(self):\n",
        "        info = 'episode = {} '.format(self.bsuite_num_episodes) + '|'\n",
        "        info += 'total return = {}'.format(int(sum(self._reward_seq))) + '|'\n",
        "        info += 'total regret = {:.2f}'.format(self._total_regret) \n",
        "        print(info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALHC9lg3bWKo"
      },
      "source": [
        "# Basis function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLPO-0-YbWKp"
      },
      "source": [
        "def random_coherent_basis(\n",
        "    size: int,\n",
        "    deterministic: bool,\n",
        "    num_basis: int,\n",
        "    rng: hk.PRNGSequence,\n",
        ") -> BasisFunction:\n",
        "    S = size\n",
        "    A = 2\n",
        "    H = S - 1\n",
        "    K = num_basis\n",
        "    if deterministic:\n",
        "        p = 1.\n",
        "    else:\n",
        "        p = (1 - 1. / S)\n",
        "    # optimal state value function\n",
        "    aux = np.array([p**t for t in range(H, 0, -1)])\n",
        "    aux = np.tril(jnp.ones(H)) @ np.diag(aux)\n",
        "    aux = np.cumsum(aux[:, ::-1], axis=1)[:, ::-1]\n",
        "    v_opt = np.concatenate([aux, aux[-1:, :]], axis=0).T\n",
        "\n",
        "    # optimal state-action value function\n",
        "    q_right = v_opt\n",
        "    q_left = np.zeros((H, S))\n",
        "    q_left[:-1, 1:] = v_opt[1:, :-1]\n",
        "    q_opt = np.stack([q_left, q_right], axis=2)\n",
        "\n",
        "    # coherent basis\n",
        "    psi_ones = jnp.ones(shape=(H * S * A, 1))\n",
        "    psi_opt = jnp.reshape(q_opt, [H * S * A, 1])\n",
        "\n",
        "    psi_rand = jax.random.normal(next(rng), shape=(H * S * A, K - 2))\n",
        "    psi = jnp.concatenate([psi_ones, psi_opt, psi_rand], axis=1)\n",
        "    proj = psi @ jnp.linalg.inv(psi.T @ psi) @ psi.T\n",
        "    w_ones = jnp.ones(shape=(H * S * A, 1))\n",
        "    w_rand = jax.random.normal(next(rng), shape=(H * S * A, K - 1))\n",
        "    w = jnp.concatenate([w_ones, w_rand], axis=1)\n",
        "    w_proj = proj @ w\n",
        "    w_proj /= jnp.linalg.norm(w_proj, axis=0)\n",
        "    w_proj *= H * S * A\n",
        "    phi = w_proj.reshape(H, S, A, K)\n",
        "\n",
        "    def basis_function(h, s, a):\n",
        "        return phi[h, s, a].squeeze()\n",
        "\n",
        "    return basis_function"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxz_WCfnbWKp"
      },
      "source": [
        "# Randomized LSVI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2zcdMr5bWKq"
      },
      "source": [
        "@partial(jax.jit, static_argnums=(2, ))\n",
        "def _lstsq(\n",
        "    A: jnp.ndarray,\n",
        "    b: jnp.ndarray,\n",
        "    reg_parameter: float,\n",
        ") -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
        "\n",
        "    # get parameters covariance\n",
        "    inv_cov = jnp.matmul(A.T, A) + reg_parameter * jnp.eye(A.shape[1])\n",
        "    cov = jnp.linalg.inv(inv_cov)\n",
        "\n",
        "    # get parameters mean\n",
        "    mean = jnp.dot(jnp.matmul(cov, A.T), b)\n",
        "\n",
        "    return mean, cov\n",
        "\n",
        "\n",
        "def _rlsvi(\n",
        "    agent_state: AgentState,\n",
        "    basis_function: BasisFunction,\n",
        "    trajectories: List[sequence.Trajectory],\n",
        "    noise_std: float,\n",
        "    reg_parameter: float,\n",
        "    sequence_length: int,\n",
        "    rng: hk.PRNGSequence,\n",
        ") -> AgentState:\n",
        "    basis_function = jax.vmap(basis_function)\n",
        "    observations = jnp.stack([t.observations for t in trajectories])\n",
        "    actions = jnp.stack([t.actions for t in trajectories])\n",
        "    rewards = jnp.stack([t.rewards for t in trajectories])\n",
        "    params_mean, params_cov, params_sample = [], [], []\n",
        "\n",
        "    for h in range(sequence_length)[::-1]:\n",
        "\n",
        "        # get regression matrix\n",
        "        batch_h = h * jnp.ones(len(observations), dtype=int)\n",
        "        batch_o = observations[:, h]\n",
        "        batch_a = actions[:, h]\n",
        "        A = basis_function(batch_h, batch_o, batch_a)\n",
        "        A /= noise_std\n",
        "\n",
        "        # get regression vector\n",
        "        batch_r = rewards[:, h]\n",
        "        batch_next_o = observations[:, h + 1]\n",
        "        if h == sequence_length - 1:\n",
        "            batch_next_r = rewards[:, -1]\n",
        "        else:\n",
        "            batch_next_q = basis_function(batch_h + 1, batch_next_o,\n",
        "                                          None) @ params_sample[-1]\n",
        "            batch_next_r = jnp.max(batch_next_q, axis=1)\n",
        "        b = batch_r + batch_next_r\n",
        "        b /= noise_std\n",
        "\n",
        "        # get parameters\n",
        "        mean, cov = _lstsq(A, b, reg_parameter)\n",
        "\n",
        "        # get sample\n",
        "        sample = jax.random.multivariate_normal(next(rng), mean, cov)\n",
        "\n",
        "        params_mean.append(mean)\n",
        "        params_cov.append(cov)\n",
        "        params_sample.append(sample)\n",
        "\n",
        "    return agent_state._replace(params_mean=params_mean[::-1],\n",
        "                                params_cov=params_cov[::-1],\n",
        "                                params_sample=params_sample[::-1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9gQonrgbWKq"
      },
      "source": [
        "# Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On3uH28SbWKr"
      },
      "source": [
        "class RLSVI(Agent):\n",
        "    def __init__(\n",
        "        self,\n",
        "        obs_spec: specs.Array,\n",
        "        action_spec: specs.DiscreteArray,\n",
        "        basis_function: BasisFunction,\n",
        "        sequence_length: int,\n",
        "        noise_std: float,\n",
        "        reg_parameter: float,\n",
        "        rng: hk.PRNGSequence,\n",
        "    ):\n",
        "\n",
        "        self._obs_spec = obs_spec\n",
        "        self._action_spec = action_spec\n",
        "        self._sequence_length = sequence_length\n",
        "        self._basis_function = basis_function\n",
        "        self._noise_std = noise_std\n",
        "        self._reg_parameter = reg_parameter\n",
        "        self._rng = rng\n",
        "\n",
        "        # initialize buffer\n",
        "        self._trajectories = []\n",
        "        self._buffer = sequence.Buffer(obs_spec, action_spec, sequence_length)\n",
        "\n",
        "        # initialize parameters\n",
        "        self._state = AgentState(None, None, None)\n",
        "\n",
        "        self._rlsvi = _rlsvi\n",
        "\n",
        "    def update(\n",
        "        self,\n",
        "        timestep: dm_env.TimeStep,\n",
        "        action: Action,\n",
        "        new_timestep: dm_env.TimeStep,\n",
        "    ):\n",
        "        self._buffer.append(timestep, action, new_timestep)\n",
        "        if self._buffer.full() or new_timestep.last():\n",
        "            trajectory = self._buffer.drain()\n",
        "            self._trajectories.append(deepcopy(trajectory))\n",
        "            self._state = self._rlsvi(self._state, self._basis_function,\n",
        "                                      self._trajectories, self._noise_std,\n",
        "                                      self._reg_parameter,\n",
        "                                      self._sequence_length, self._rng)\n",
        "\n",
        "    def select_action(\n",
        "        self,\n",
        "        timestep: dm_env.TimeStep,\n",
        "    ) -> Action:\n",
        "        if self._state.params_sample == None:\n",
        "            action = jax.random.randint(next(self._rng), (), 0,\n",
        "                                        self._action_spec.num_values)\n",
        "            return int(action)\n",
        "        h = int(timestep.step_type)\n",
        "        o = timestep.observation\n",
        "        basis = jnp.stack([\n",
        "            self._basis_function(h, o, a)\n",
        "            for a in range(self._action_spec.num_values)\n",
        "        ],\n",
        "                          axis=0)\n",
        "        params = self._state.params_sample[h]\n",
        "        action = jnp.argmax(basis @ params)\n",
        "        return int(action)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4z3WEzYbWKr"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zSQZ6ghbWKr"
      },
      "source": [
        "seed = 1234 # Random seed\n",
        "size = 20 # Size of chain experiment\n",
        "deterministic = False # Deterministic environment\n",
        "num_basis = 10 # Number of basis functions\n",
        "noise_std = 0.1 # Standard deviation of RLSVI\n",
        "reg_parameter = 1. # Regularization parameter of RLSVI\n",
        "num_episodes = 300 # Number of train episodes\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7WpVcSnbWKs"
      },
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1igTNMxbWKt"
      },
      "source": [
        "key = jax.random.PRNGKey(seed)\n",
        "rng = hk.PRNGSequence(key)\n",
        "\n",
        "# set environment\n",
        "env = ChainEnv(size=size, deterministic=deterministic)\n",
        "\n",
        "# set basis function\n",
        "basis_function = random_coherent_basis(size=size,\n",
        "                                        deterministic=deterministic,\n",
        "                                        num_basis=num_basis,\n",
        "                                        rng=rng)\n",
        "\n",
        "# set agent\n",
        "agent = RLSVI(obs_spec=env.observation_spec(),\n",
        "                action_spec=env.action_spec(),\n",
        "                basis_function=basis_function,\n",
        "                sequence_length=size,\n",
        "                noise_std=noise_std,\n",
        "                reg_parameter=reg_parameter,\n",
        "                rng=rng)\n",
        "\n",
        "# run experiment\n",
        "experiment.run(agent=agent,\n",
        "                environment=env,\n",
        "                num_episodes=num_episodes,\n",
        "                verbose=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tY_yOMyTget8"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNEobLrubkyc"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "x = range(env.bsuite_num_episodes)\n",
        "y = np.cumsum(env._regret_seq)\n",
        "\n",
        "ax.plot(x,y)\n",
        "\n",
        "ax.set_xlabel('Episodes')\n",
        "ax.set_ylabel('Cumulative regret')\n",
        "ax.set_title('Chain with {} states'.format(size))\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}